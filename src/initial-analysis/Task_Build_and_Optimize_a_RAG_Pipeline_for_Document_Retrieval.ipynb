{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0So_mk08pdn0"
   },
   "source": [
    "## Building a RAG pipeline\n",
    "\n",
    "This notebook implements a full RAG pipeline, starting with PDF parsing, chunking, embedding, information retrieval, and reranking to optimize the quality of search results from the Lender Fees PDF Document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xj7nulQ_FqDk"
   },
   "source": [
    "## Installing Python packages into my local environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C6ppJILpFml1",
    "outputId": "927979e6-4969-4882-d95b-4634fdb976e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m85.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.9/11.9 MB\u001b[0m \u001b[31m106.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m303.3/303.3 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m99.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m63.9/63.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m323.9/323.9 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m88.0/88.0 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m144.4/144.4 kB\u001b[0m \u001b[31m12.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "ipython 7.34.0 requires jedi>=0.16, which is not installed.\u001b[0m\u001b[31m\n",
      "\u001b[0mCollecting llama-index-retrievers-bm25\n",
      "  Downloading llama_index_retrievers_bm25-0.6.5-py3-none-any.whl.metadata (446 bytes)\n",
      "Collecting bm25s>=0.2.7.post1 (from llama-index-retrievers-bm25)\n",
      "  Downloading bm25s-0.2.14-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: llama-index-core<0.15,>=0.13.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-retrievers-bm25) (0.14.5)\n",
      "Collecting pystemmer<3,>=2.2.0.1 (from llama-index-retrievers-bm25)\n",
      "  Downloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (1.16.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from bm25s>=0.2.7.post1->llama-index-retrievers-bm25) (2.0.2)\n",
      "Requirement already satisfied: aiohttp<4,>=3.8.6 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.13.1)\n",
      "Requirement already satisfied: aiosqlite in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.21.0)\n",
      "Requirement already satisfied: banks<3,>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.2.0)\n",
      "Requirement already satisfied: dataclasses-json in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.6.7)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.18)\n",
      "Requirement already satisfied: dirtyjson<2,>=1.0.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.8)\n",
      "Requirement already satisfied: filetype<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.3.0)\n",
      "Requirement already satisfied: httpx in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.28.1)\n",
      "Requirement already satisfied: llama-index-workflows<3,>=2 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.8.3)\n",
      "Requirement already satisfied: nest-asyncio<2,>=1.5.8 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.5)\n",
      "Requirement already satisfied: nltk>3.8.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.9.1)\n",
      "Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (10.4.0)\n",
      "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.5.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.11.10)\n",
      "Requirement already satisfied: pyyaml>=6.0.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.0.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.32.4)\n",
      "Requirement already satisfied: setuptools>=80.9.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (80.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.49 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.0.44)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.5.0)\n",
      "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.12.0)\n",
      "Requirement already satisfied: tqdm<5,>=4.66.1 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.15.0)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.9.0)\n",
      "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.17.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.4.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp<4,>=3.8.6->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.22.0)\n",
      "Requirement already satisfied: griffe in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.14.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.1.6)\n",
      "Requirement already satisfied: llama-index-instrumentation>=0.1.0 in /usr/local/lib/python3.12/dist-packages (from llama-index-workflows<3,>=2->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.2)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (8.3.0)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.5.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>3.8.1->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (2025.10.5)\n",
      "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.49->sqlalchemy[asyncio]>=1.4.49->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.2.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from typing-inspect>=0.8.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.1.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.26.1)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (4.11.0)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.16.0)\n",
      "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.12/dist-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (25.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (1.3.1)\n",
      "Requirement already satisfied: colorama>=0.4 in /usr/local/lib/python3.12/dist-packages (from griffe->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->banks<3,>=2.2.0->llama-index-core<0.15,>=0.13.1->llama-index-retrievers-bm25) (3.0.3)\n",
      "Downloading llama_index_retrievers_bm25-0.6.5-py3-none-any.whl (5.0 kB)\n",
      "Downloading bm25s-0.2.14-py3-none-any.whl (55 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m55.1/55.1 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading PyStemmer-2.2.0.3-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (683 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m683.3/683.3 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pystemmer, bm25s, llama-index-retrievers-bm25\n",
      "Successfully installed bm25s-0.2.14 llama-index-retrievers-bm25-0.6.5 pystemmer-2.2.0.3\n"
     ]
    }
   ],
   "source": [
    "!pip install -q llama-index llama-index-llms-gemini pymupdf\n",
    "!pip install -q llama-index-embeddings-huggingface\n",
    "!pip install llama-index-retrievers-bm25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Txd9y-PsGELc"
   },
   "source": [
    "## Installing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DqGDnlksGGT5"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import fitz\n",
    "import os\n",
    "from llama_index.core import Document\n",
    "from typing import List\n",
    "from llama_index.llms.gemini import Gemini\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core.node_parser import SemanticSplitterNodeParser\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import VectorStoreIndex\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import BaseRetriever\n",
    "from llama_index.core.schema import NodeWithScore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2Z-CRPGYHpLX"
   },
   "source": [
    "## Set up Google API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ptgoy12sH_MW"
   },
   "outputs": [],
   "source": [
    "GOOGLE_API_KEY = \"\"\n",
    "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "19pdUrIVIHIo"
   },
   "source": [
    "## Load PDF and convert to an Llama-Index Compatible / Parseable Format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zSMiXKvTy75N"
   },
   "source": [
    "1. Open the PDF: Uses PyMuPDF (fitz.open) to open the PDF file you give it.\n",
    "\n",
    "2. Go through each page: Loops through the pages one by one.\n",
    "\n",
    "3. Grab the text: For each page, it pulls out the text. If a page has no text (blank or images only), it skips it.\n",
    "\n",
    "4. Make a document object: For pages that do have text, it creates a Document object. This object stores:\n",
    "    - The page‚Äôs text\n",
    "    - Some extra info (metadata): the PDF‚Äôs file name, the page number, and the total number of pages in the PDF.\n",
    "\n",
    "5. Keep all pages together: Adds each of these Document objects into a list.\n",
    "\n",
    "6. Close the PDF: After reading everything, it closes the PDF file.\n",
    "\n",
    "7. Tell you what it did: Prints out the file name, how many pages it looked at, and how many of those pages had text.\n",
    "\n",
    "8. Return the result: Finally, it gives back the list of all Document objects with text and metadata.\n",
    "\n",
    "üëâ In short: This function takes a PDF file, pulls the text out of each page, attaches some basic info (like page number), and returns it as a list of neat, ready-to-use objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z6zKnVH3Ie8p"
   },
   "outputs": [],
   "source": [
    "def load_pdf(pdf_path: str) -> List[Document]:\n",
    "    \"\"\"Load a PDF and convert it to LlamaIndex Document format using PyMuPDF.\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    documents = []\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        if not text.strip():\n",
    "            continue\n",
    "        documents.append(\n",
    "            Document(\n",
    "                text=text,\n",
    "                metadata={\n",
    "                    \"file_name\": os.path.basename(pdf_path),\n",
    "                    \"page_number\": i + 1,\n",
    "                    \"total_pages\": len(doc)\n",
    "                }\n",
    "            )\n",
    "        )\n",
    "    doc.close()\n",
    "    print(f\"Processed {pdf_path}:\")\n",
    "    print(f\"Extracted {len(documents)} pages with content\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3EXPhkdG35F"
   },
   "source": [
    "## Initialize Gemini and Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHQssX32zUB8"
   },
   "source": [
    "1. Initialize Gemini LLM:\n",
    "  - Creates a Gemini LLM using the \"gemini-2.0-flash\" model.\n",
    "  - Tells the Settings object: ‚ÄúUse this LLM for all future tasks.\n",
    "\n",
    "2. Set up the embedding model:\n",
    "  - Loads a HuggingFace embedding model called \"BAAI/bge-small-en\".\n",
    "\n",
    "**Note:** `bge-small-en` is an efficient embedding model developed by BAAI that transforms English text into 384-dimensional vectors (embeddings) for semantic search and other tasks. It is a small, fast, and high-performing model based on transformer architecture, optimized for both similarity matching and retrieval using contrastive learning. Key features include its small size, strong performance on the MTEB benchmark, and special instruction-based handling for retrieval tasks.\n",
    "\n",
    "  - An embedding model turns text into number vectors so the system can compare meaning between pieces of text.\n",
    "  - Then it tells Settings: ‚ÄúUse this embedding model going forward.\"\n",
    "\n",
    "3. Create a semantic text splitter:\n",
    "  - This tool takes a big chunk of text (like a PDF or long document) and splits it into smaller pieces.\n",
    "  - Instead of cutting at random places (like fixed length), it uses the embedding model to check for changes in meaning and splits where topics shift.\n",
    "  \n",
    "    - buffer_size = 1 ‚Üí keeps a small overlap of sentences around the split so context isn‚Äôt lost.\n",
    "      - With 1, the splitter copies 1 sentence from before and after each cut into neighboring chunks so context isn‚Äôt lost.\n",
    "    - breakpoint_percentile_threshold = 95 ‚Üí controls how ‚Äúsensitive‚Äù it is to meaning changes.\n",
    "      - The splitter scores how much the meaning changes between neighboring sentences (using embeddings). It then takes the 95th-percentile of those change scores and cuts only at the biggest jumps (roughly the top 5% of changes). If you lower it (e.g., 80), you get more, smaller chunks; raise it (e.g., 98) for fewer, bigger chunks.\n",
    "        - Higher = only splits when the meaning shift is very strong.\n",
    "        - Lower = splits more often, even on smaller changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528,
     "referenced_widgets": [
      "56f9f65816e0412c9a9ca658dce0df88",
      "808acef6d8e0495e8c59006f556f24c9",
      "287a3c03bcc74ff4b976ebd7a5e1864f",
      "470c812aac014f79a261b86bee8cfe88",
      "f2b19501507b413a93d717d8d13ba39b",
      "44b763b042fb4acf9929a46041998031",
      "2d96ec6d222b468b9ebbc987e7ce2452",
      "625739e1701a4f57850d5032bf8920a7",
      "b8f88cf1c227447eb00e98b13803459b",
      "97bf9053367b43f2bf64b03b53eb07a7",
      "3573a398cf8e4744abb9fac9a87f06b7",
      "740300ea7c4646f6bbec810cf9d219f7",
      "558c0b508d164c5dafbc06111133ac5a",
      "b652ac5386734e0390bb92f9a68db9cc",
      "f6e8e58a41f84f7fa5cacf621a6464e3",
      "878e551eca4544fca19a50ea787c24c4",
      "e296f9470dde4baebf9341597e9412ed",
      "11f8dd573f0b4460b7f13a4c7c1f4bca",
      "ca7f92a1dcd741abab99cb83fd4b69e5",
      "c7fae46020e14fd0917e7d4b8339854b",
      "2aaef32e5eac41cb9f1777732973ff08",
      "12ad3ecfcb904aae94a2cae7da83752a",
      "d4f62c0fb9f6464cae0e16e1bdb66fb3",
      "e9a33a4d8b69488887f7f87274f81c25",
      "30e5b8be835c4a63a0987cb7109d5fd2",
      "cf4134f5c0b94f399cbeeb11cd6e99d0",
      "1840d6b6d6624109a203d3469348fa02",
      "71ae49a91fa44457960cd2563351e654",
      "06fc94b5dffc467a9a7ffbaac607ddc9",
      "8a32f8b8b8bf4e98baa3d0a5e57800e5",
      "7868e7829e17479fa7e5aca77a0c37dd",
      "385e1d78f3b642dd8d2c63b1857bbdac",
      "254b1f3bf0a448efaa68372a15db7ad5",
      "c90d0d73e25c472491dd796427d16955",
      "986ae12eb35b461c86dac72d323fa020",
      "aa7eb2ce06754961b8b48a78b3613655",
      "f4982686e02b4b54aa41489ab89a83d8",
      "0f812aacbc53485ea7ba16b2ceb16c11",
      "589d70016296466a80ce0cae5188dd0f",
      "d63ee993ad544564871651116be1d7bb",
      "2c83e4e508ee44f69fa7bf6c7f748fa5",
      "9d143a019bee457e9eeef9d47365219c",
      "4c24b9e1e5cc4b779baba77407d857e2",
      "603a736f2cd84647a76da35f03d0fa42",
      "eff5292298c044288d488cedef37f87e",
      "e09f2d69a96c4f4ba5695cd80f9b9555",
      "6907437434d94ae9b4650e94ac73c172",
      "f24836b4c94f4585918e4b9c9981bb61",
      "170afcec65b247959e3b83a70b84c9af",
      "89646f80f9704a889bc507c89fbf93c1",
      "bffc6daa03bc472695b9394a64b14074",
      "e6ca652866824cfabd44ee1d2dbc941f",
      "1a7d7e13f52f42e88acd0f144d59fba7",
      "4f249c19b7814c20b268f52770d8a8aa",
      "1598a0185fc541bb88e8933cc7a909d9",
      "a1450fc8333048eda9b18f84d6de94c2",
      "c4626ec942ee477d86ba46dee19c29cb",
      "a488c5e9d89c48329abd03d5583b25a2",
      "adf649f917fb43e38a1505718e6bc835",
      "a12a0751be9840a5b7aafadf5d4b61a0",
      "478c8137a7764db18e5df9009326f1bc",
      "db891447141642edbcbe55ecff32f164",
      "e961e49c825046a381b11559ad591c15",
      "abf8c3473d3d4a1fb12c431a31aae2ec",
      "79430f441d7247dc9b445ba4c179ec5d",
      "33b07ca7b9c24b34ace425a8e4bf8bf0",
      "3234e6a368fc4cc5846ab204398eb05e",
      "cac27830dde946f3926faa51a690197d",
      "ad6ce53f415940ef94456d90912e75f2",
      "636ce270b66c4edf9e7726f9e295e4c0",
      "22a5f4ab77674bf585c9c8e9ed61736f",
      "0aded73f76774b62b9a00cf126445a6c",
      "e64692e9f0514589815baa219328af5d",
      "9136d104db2d441fa55faf14a492c13e",
      "eb7361a212a047c2b9f304c0a3ff3647",
      "d7f35bdd3abf493486f52657fbbb30c0",
      "5e49a38b8e8d4eae9eb7b728fbc10c1b",
      "ea6dcb0b739c429a9e82774e0cb9e12d",
      "032d32f78ba249ed96e7ae73047ec8d2",
      "2e3f092d1f8f426aad9422c22172304f",
      "aa9580e36cb74983b82374671b45f3c5",
      "cb9c08e348db49929534ab8a15c20027",
      "78dccf0ce8344b4a8082eb858ab4aeec",
      "ffc53086abb94fd1aa7f2b3cf4c8ecb4",
      "9b9099a7174a4251b9520525f8866999",
      "65177e69c0b648e0937def4e66c17ff5",
      "fe8b88551a8d4f5699dd9a230ed6fd3c",
      "5e5f3de6e4474692b4bb4f3e77365ed0",
      "5524cea876b14141a641c3c1a89cc521",
      "aa9fbad1460242f79ea8d13a507e8111",
      "a01a9c58a2474bc6887edfe5a665cf2e",
      "cd8f8caa543d4f21b2bfdadb8c62ac08",
      "6948bc945a364e06ad903b0052d85ae3",
      "ebb5ab03d0454dcb992c3721f533bc9f",
      "43c4322384f8466d99327a6cec8c1c12",
      "9bb392b61091499391a83b810cc772c5",
      "898aa6aeaacc480fad72abd175c2ff55",
      "e1b39eafb38a4dd3af9ad307e2d34314",
      "5027266b5b2544d4abd78f712beb460c",
      "2f410f422553456097d80a8850bd7e4b",
      "98795382b7814f1bb17e54987b134635",
      "2d4ee90bd02c497aa490bbd74db05ef5",
      "fee0eea3347e4c10a896c0a66ad358a8",
      "9974748135984060ba10a69116bf0ced",
      "81a4ad55fc264cdd8e460a67ffd883fa",
      "50d80810ea5148bb80263b0e7e2a08fb",
      "c73be165f934467dbbff1367490181c8",
      "2d088e868b39438aa0e09fef01d4acf0",
      "742731082efd4e25999bf6316fa83eb5",
      "26b46208523c4b89ad2c57e58d9d04e4",
      "8f460cd5ebf44bf6b523f8c0cdd74e53",
      "12138cb8d14448138fee11d4e52de163",
      "93d94f65a88e42fa930ff999002df7c5",
      "761e64c6d5d44732a6e309668c07cacc",
      "b132edf80ac64f99a5c3ce8dbab5a0ea",
      "4c7f422dd05b4251a05f2cc6cb6fa2a3",
      "51c5ffcb8b704e03a2738fed35c44de5",
      "c1319a0aa39d48e782590beb956fdf86",
      "035fd278b5734ead91b66b219c125c86",
      "97c101f97a734ad59c114cb54b7568ed",
      "48932f8ec67c48e5ae047dab1858066b"
     ]
    },
    "id": "C8AL4669G68m",
    "outputId": "544ea352-bceb-439b-f0fb-c8b3bba4c2ef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-688776803.py:2: DeprecationWarning: Call to deprecated class Gemini. (Should use `llama-index-llms-google-genai` instead, using Google's latest unified SDK. See: https://docs.llamaindex.ai/en/stable/examples/llm/google_genai/)\n",
      "  llm = Gemini(model=\"models/gemini-2.0-flash\")\n",
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56f9f65816e0412c9a9ca658dce0df88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "740300ea7c4646f6bbec810cf9d219f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f62c0fb9f6464cae0e16e1bdb66fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c90d0d73e25c472491dd796427d16955",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/52.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eff5292298c044288d488cedef37f87e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1450fc8333048eda9b18f84d6de94c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3234e6a368fc4cc5846ab204398eb05e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/366 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6dcb0b739c429a9e82774e0cb9e12d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5524cea876b14141a641c3c1a89cc521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f410f422553456097d80a8850bd7e4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f460cd5ebf44bf6b523f8c0cdd74e53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize Gemini LLM\n",
    "llm = Gemini(model=\"models/gemini-2.0-flash\")\n",
    "Settings.llm = llm\n",
    "\n",
    "# Initialize embedding model\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-small-en\")\n",
    "Settings.embed_model = embed_model\n",
    "splitter = SemanticSplitterNodeParser( # Creates semantic splitter with embedding model\n",
    "    buffer_size = 1,\n",
    "    breakpoint_percentile_threshold = 95, # How sensitive to change in meaning\n",
    "    embed_model = embed_model\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TE1g4qd6HEnH"
   },
   "source": [
    "## Pre-process PDF, and create vector and keyword indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QtP0ZEA3bqK"
   },
   "source": [
    "This function takes a PDF, splits it into smart chunks, converts those chunks into embeddings, and builds a semantic search index you can query later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ULjdpGv2HCJm"
   },
   "outputs": [],
   "source": [
    "def process_and_index_pdf(pdf_path):\n",
    "    documents = load_pdf(pdf_path)\n",
    "    nodes = splitter.get_nodes_from_documents(documents)\n",
    "    vector_index = VectorStoreIndex(nodes)\n",
    "    print(f\"Indexed {len(documents)} document chunks\")\n",
    "    return vector_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HxWukDS0Hkx8"
   },
   "source": [
    "## Build RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Al6G4Jts3keZ"
   },
   "source": [
    "1. Collect the text chunks: Pulls every chunk of text that was created when you indexed the PDF and counts them.\n",
    "\n",
    "2. Pick how many results to fetch (‚Äútop-k‚Äù):\n",
    "- If there‚Äôs only 1 chunk, fetch 1.\n",
    "- If there are 2 or more, fetch 2.\n",
    "\n",
    "*(That‚Äôs what safe_top_k ensures.)*\n",
    "\n",
    "3. Set up two ways to find relevant chunks:\n",
    "- Vector search: finds chunks that mean something similar to the question.\n",
    "- Keyword search (BM25): finds chunks that share exact words with the question.\n",
    "\n",
    "4. Combine them (HybridRetriever):Runs both searches, merges the results, removes duplicates, sorts by score (best first), and keeps only the top k.\n",
    "\n",
    "5. (Optional) Re-rank with a smarter model:\n",
    "If there‚Äôs more than one chunk total, it uses a cross-encoder reranker to double-check which of the top candidates best match the question, and reorders them.\n",
    "\n",
    "6. Broaden the search with query fusion:\n",
    "- Asks the LLM to rewrite the user‚Äôs question in a few different ways (3 variants)\n",
    "- Searches with each\n",
    "- Blends the results so you don‚Äôt miss answers due to phrasing variations.\n",
    "\n",
    "7. Build the final Q&A engine: Wires the fusion retriever (plus the optional reranker) into a query engine that you can call with a question to get the best matching chunk(s) and an answer.\n",
    "\n",
    "8. Return the result: You get back a ready-to-use query_engine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eIqY-BaLIk86"
   },
   "outputs": [],
   "source": [
    "def build_rag_pipeline(index):\n",
    "    nodes = list(index.docstore.docs.values()) # Gets all chunks of text that were created when PDF was indexed\n",
    "    num_nodes = len(nodes) # Stores how many chunks there are\n",
    "    safe_top_k = min(2, max(1, num_nodes)) # Retrieves the minimum value for top k\n",
    "\n",
    "    vector_retriever = index.as_retriever(similarity_top_k=safe_top_k) # Uses embeddings to find chunks that are semantically similar\n",
    "    bm25_retriever = BM25Retriever.from_defaults( # Uses keyword search to find exact terms in chunks found in the query\n",
    "        nodes=nodes,\n",
    "        similarity_top_k=safe_top_k\n",
    "    )\n",
    "\n",
    "    class HybridRetriever(BaseRetriever): # Custom class to combine both vector and keyword search\n",
    "        def __init__(self, vector_retriever, keyword_retriever, top_k=2):\n",
    "            self.vector_retriever = vector_retriever\n",
    "            self.keyword_retriever = keyword_retriever\n",
    "            self.top_k = top_k\n",
    "            super().__init__()\n",
    "\n",
    "        def _retrieve(self, query_bundle, **kwargs):\n",
    "            vector_nodes = self.vector_retriever.retrieve(query_bundle)\n",
    "            keyword_nodes = self.keyword_retriever.retrieve(query_bundle)\n",
    "            all_nodes = list(vector_nodes) + list(keyword_nodes)\n",
    "            unique_nodes = {node.node_id: node for node in all_nodes}\n",
    "            sorted_nodes = sorted(\n",
    "                unique_nodes.values(),\n",
    "                key=lambda x: x.score if hasattr(x, 'score') else 0.0,\n",
    "                reverse=True\n",
    "            )\n",
    "            return sorted_nodes[:self.top_k]\n",
    "\n",
    "    hybrid_retriever = HybridRetriever( # Creates instance of class defined above\n",
    "        vector_retriever=vector_retriever,\n",
    "        keyword_retriever=bm25_retriever,\n",
    "        top_k=safe_top_k\n",
    "    )\n",
    "\n",
    "    if num_nodes > 1:\n",
    "        reranker = SentenceTransformerRerank( # Checks which chunk is most relevant to original query\n",
    "            model=\"cross-encoder/ms-marco-MiniLM-L-12-v2\", # More powerful than L-6 version\n",
    "            top_n=min(2, num_nodes)\n",
    "        )\n",
    "        node_postprocessors = [reranker]\n",
    "    else:\n",
    "        node_postprocessors = []\n",
    "\n",
    "    fusion_retriever = QueryFusionRetriever( # Creates multiple versions of the user's query\n",
    "        retrievers=[hybrid_retriever],\n",
    "        llm=llm,\n",
    "        similarity_top_k=2,\n",
    "        num_queries=3,  # Generate 3 queries per original query\n",
    "        mode=\"reciprocal_rerank\"\n",
    "    )\n",
    "\n",
    "    query_engine = RetrieverQueryEngine.from_args( # Takes fusion retriever and reranker and combines them\n",
    "        retriever=fusion_retriever,\n",
    "        llm=llm,\n",
    "        node_postprocessors=node_postprocessors\n",
    "    )\n",
    "    return query_engine # Returns output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9VCQg4kIoJf"
   },
   "source": [
    "## Upload PDF Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 90
    },
    "id": "4275GmkcIn6M",
    "outputId": "9f831efc-9fa4-468d-ab18-7950240518af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please select a PDF file to upload.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-136c71fb-6046-443b-afe4-e023ab19e355\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-136c71fb-6046-443b-afe4-e023ab19e355\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving Sriram Srinivasan Data Analyst Resume.pdf to Sriram Srinivasan Data Analyst Resume.pdf\n"
     ]
    }
   ],
   "source": [
    "print(\"Please select a PDF file to upload.\")\n",
    "uploaded = files.upload()\n",
    "pdf_path = list(uploaded.keys())[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ovC6SCuKIwWu"
   },
   "source": [
    "## Run the query using the RAG pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/",
     "height": 659,
     "referenced_widgets": [
      "303cb26e5aa649f0b4217672385837b4",
      "9dcda85eeb76483b85bee80e58f86e07",
      "97c04c5835c24c5581b2ae8ff2cffc8d",
      "19dab2c45e8043a48ebb6fcd059b7373",
      "86f548fedd594f3893a4cf2a746a84e1",
      "58981cb132eb4f88be02821c2e602c6e",
      "7024029293124a59b3ae22a35fba5434",
      "4c05986fedc741cf8d2a5b343ef72ea3",
      "29ec292969af42998ac3d777b9a79b4e",
      "6c8d18f143e948b99d1ff3c41fb54d50",
      "c9e74f803fe04f09a05f03edb35bdd4d",
      "e5dca7298d834fc5b0e5ecff5889537a",
      "2e47d5215f484e7ea69e1fd62d3d0f19",
      "28284f0936b2497bb280b44cca874466",
      "dace44264f8847a49ffccc231a1a6160",
      "db988d113b8a4117aa11b64e71d3374f",
      "bc08ad3fbca741bca2618cd6f7e3fe7d",
      "74b721d674b94657a45dcf61ae5542e1",
      "459982e4baa9491cabea308d0f89bba0",
      "81a4db9862ab449bad0e51349d0b1d8b",
      "ab203c2bc5e44bcb99f41c1183996a38",
      "13a09b194da34feeb51b730b5e4b88b8",
      "5e3d1c32ee14495aa7b11333d395cc0e",
      "a31905c427a44e2faaf07db1a017b8a2",
      "96ed973148a14efaa7bd265b0f853daa",
      "6428d800680042c8a3bc43d0e8521eee",
      "fdcb2bce218946409d0e3a22ec7a56ad",
      "b87e618778f6415aafc9a3a5989ee43f",
      "12e19190eb6f4935a9c96ca2a3320062",
      "53666d65c5a7459d8eed732dc1d95515",
      "8fbf08d2522a41ccb101040b4d3a8525",
      "8d6d4bb0d7fe4f1abcc56138be372484",
      "88658bf7582a41e8866f94d29353922a",
      "7e488c511b214bdba93fefed1279fb8c",
      "171849fe805d4a2ab5d462e04f48bdea",
      "f0a4d11749c74224bb73746d5eb9c859",
      "7e057ad3a29f4a068bbffe1067ecd100",
      "365ecfc113b34129a83aa4805b91faf9",
      "afcd11b5adcd4272b6c2f6ceefdef79d",
      "b6f874a1b1984ef4a1075ff3433a1959",
      "f46d4bf8b8d047dca916752ac497b2fe",
      "e38cd0aab9424fb18877954e36aff6ed",
      "770ff4cd21164c88a9c5cc68924ae5ef",
      "5447dd153a2f4f5b8259feb24ebba8a9",
      "8e6b89c6eb27440e84869210b144c273",
      "01faebc941f3454d854b42f5119ccd8b",
      "26088750ba924bcf84982d950b85883b",
      "c0be4a877d594fec8cb084fb54994a7f",
      "51198786cb6744c29e276d92c5eccb8c",
      "6a80a9e5525c484fae2de9304786fe4f",
      "26bedc1335304d8e8d06265d96be7157",
      "71037b8c3c8a4d89b92b808c70b71d9c",
      "8aae9ddfb21c4c13abd9937d09cd0f90",
      "9f98db44504d4873bd7e6455ebed0cb0",
      "39dc632e50f2423a984a2b39b5dce0b0",
      "19ac10d4beac458cb4be2ec9f589cf17",
      "73c4ca7c5f394942860ac8753483e22b",
      "2337b61a1d26406ba0b8efb8dd8d94b3",
      "5f0982d7231948c5ab2c7e940d6a0e3f",
      "b940e6b7688b4d7ab830512f845632ab",
      "f18ee66350e1478bbc8ee5c302ccd1c8",
      "c0ac04b8a7c04c3094ae2aba8f1ba18d",
      "2267ebfecca24880a64df2b5383cfc83",
      "710127b30e4e4103b28508b1c81e70d8",
      "1e1b998f9dcd42cdb668a62b7cc438db",
      "bc9145d14a5849ed97013668ea41969b",
      "17d5774308fc4b39b52c7b3c3c125f41",
      "d6733b2c63bf4e1fb3d0859b4bcb5610",
      "1adcf2790f924fff8fa09b288390b323",
      "d60dc233daf24abcb327356d241cc618",
      "fb78a968d019407d84567f725c614d16",
      "0fcb34b44f504750879d43b7d0f29b0f",
      "10cb6485c47a432e8db4ec00350041a4",
      "4228daac502e4bc1b81d510505ea5888",
      "311f28e35cdd453dba79059d9320c9a5",
      "81df7f5c8b9e46658d5371bd29786e56",
      "cd83edc29ce94dbeb4b14c76d6de0e18"
     ]
    },
    "id": "6LQDNk-eIzcd",
    "outputId": "4de1d26e-57d5-4bcd-b6d3-b8fbd276d24b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed Sriram Srinivasan Data Analyst Resume.pdf:\n",
      "Extracted 1 pages with content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:bm25s:Building index from IDs objects\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexed 1 document chunks\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303cb26e5aa649f0b4217672385837b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/791 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5dca7298d834fc5b0e5ecff5889537a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/133M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3d1c32ee14495aa7b11333d395cc0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7e488c511b214bdba93fefed1279fb8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e6b89c6eb27440e84869210b144c273",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19ac10d4beac458cb4be2ec9f589cf17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17d5774308fc4b39b52c7b3c3c125f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type 'Exit' to stop.\n",
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "Sriram Srinivasan\n",
      "\n",
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "I graduated with a Master of Science in Computer Science in May 2024 and a Bachelor of Science in Computer Science in May 2022.\n",
      "\n",
      "\n",
      "Final Response:\n",
      " ---------------------- \n",
      "\n",
      "The latest employer is TruBridge, where the individual works as a Healthcare Data Analyst.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "index = process_and_index_pdf(pdf_path)\n",
    "rag_engine = build_rag_pipeline(index)\n",
    "print(\"Type 'Exit' to stop.\")\n",
    "while True:\n",
    "  user_input = input()\n",
    "  if user_input == 'Exit':\n",
    "    break\n",
    "  response = rag_engine.query(user_input)\n",
    "  print('\\nFinal Response:\\n ---------------------- \\n')\n",
    "  print(response)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
