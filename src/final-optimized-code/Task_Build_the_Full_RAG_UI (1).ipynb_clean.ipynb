{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rAIDsUZ1R8_-"
   },
   "source": [
    "## üìö Setup & Installation\n",
    "\n",
    "1. Installs/updates core libraries for your stack:\n",
    "*   `llama-cpp-python` (CUDA wheel), `gradio`, `gradio_pdf`, `pymupdf`, `PyPDF2`, `pillow`, `pytesseract` <br>\n",
    "*   `sentence-transformers` for embeddings, `faiss-cpu` for vector search\n",
    "\n",
    "2. Prints CUDA + model backend info to confirm GPU acceleration is active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tRdhWXVEw1AK",
    "outputId": "7b002dcc-7205-4e16-9902-eb99594ddb81"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/1.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r",
      "\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Clean any CPU-only build first\n",
    "# !pip -q uninstall -y llama-cpp-python\n",
    "\n",
    "# Upgrade pip\n",
    "!pip -q install --upgrade pip\n",
    "\n",
    "# Install CUDA-enabled wheel (pick ONE of these)\n",
    "!pip -q install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124\n",
    "# or\n",
    "# !pip -q install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu125"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pqFG6yj5yp2U",
    "outputId": "f432b8a6-a3c5-46c3-fd50-377977519125"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_cuda_init: GGML_CUDA_FORCE_MMQ:    yes\n",
      "ggml_cuda_init: GGML_CUDA_FORCE_CUBLAS: no\n",
      "ggml_cuda_init: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n"
     ]
    }
   ],
   "source": [
    "import llama_cpp, sys\n",
    "print(llama_cpp.llama_print_system_info().decode(\"utf-8\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "aoz3IE7k0Tyn"
   },
   "outputs": [],
   "source": [
    "# Intalling UI / PDF / OCR deps\n",
    "!pip -q install gradio gradio_pdf\n",
    "!pip -q install pymupdf PyPDF2\n",
    "!pip -q install pillow pytesseract\n",
    "\n",
    "# Text embeddings (runs fine on either CPU or GPU via PyTorch)\n",
    "!pip -q install sentence-transformers\n",
    "\n",
    "# Vector store ‚Äî faiss-cpu is the most reliable on Colab CUDA 12.x\n",
    "!pip -q install faiss-cpu\n",
    "\n",
    "# IMPORTANT: don't reinstall llama-cpp-python here.\n",
    "# If you ever need to, always use the CUDA wheel again:\n",
    "# !pip -q install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g9BXhrAs08Hp",
    "outputId": "98df8733-9c34-4fb4-8fb9-a6f8faf59828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch cuda: True\n",
      "CUDA : ARCHS = 500,520,530,600,610,620,700,720,750,800,860,870,890,900 | FORCE_MMQ = 1 | USE_GRAPHS = 1 | PEER_MAX_BATCH_SIZE = 128 | CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n"
     ]
    }
   ],
   "source": [
    "import torch, llama_cpp\n",
    "print(\"torch cuda:\", torch.cuda.is_available())\n",
    "print(llama_cpp.llama_print_system_info().decode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XI0CIKuEbvNo"
   },
   "source": [
    "## ‚¨áÔ∏è Download Model (Mistral-7B-Instruct GGUF)\n",
    "\n",
    "*   Prepares a `/content/models` directory and downloads Mistral-7B open-source LLM model\n",
    "*   By doing this, you get a local path for fast, private inference with `llama.cpp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b2LIFNogby1B",
    "outputId": "b930fd35-8a24-4a6a-8047-6071cfb76f75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2026-01-23 00:52:56--  https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf?download=1\n",
      "Resolving huggingface.co (huggingface.co)... 13.35.202.34, 13.35.202.121, 13.35.202.97, ...\n",
      "Connecting to huggingface.co (huggingface.co)|13.35.202.34|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://us.gcp.cdn.hf.co/xet-bridge-us/65778ac662d3ac1817cc9201/865f5e4682dddb29c2e20270b2471a7590c83a414bbf1d72cf4c08fdff2eeca4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&Expires=1769133176&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5MTMzMTc2fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjU3NzhhYzY2MmQzYWMxODE3Y2M5MjAxLzg2NWY1ZTQ2ODJkZGRiMjljMmUyMDI3MGIyNDcxYTc1OTBjODNhNDE0YmJmMWQ3MmNmNGMwOGZkZmYyZWVjYTRcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=iKdv%7EZld0Lqy96gvhfVnAcELXhcZ7wiza5QdCcN1giyJxEfZqBxvTBs6bpox4E7HPx%7EP9EZfgrqbVVn5GnwBHaYUrJk302xA0VsIvO4gBGymY9gXmOV0ptjyhv1BdjFVBFy%7EE8cG8KwYw-dsTg5hedt2433yHOXACgAJh-oNCHUCzqOqLbbu5l7s7JN7Jsd0q410Ik848pTALvKLVmO24liLv7Z79Rx3YnfRr878UjV0nYF2ouytIsc9ktLfbZQzTGz3d%7EEYeLtKX38KzBQFXvdk%7ELnkzxP45D2GvyF690U4T2nSGk94jqW42nbDmr06RxsxzlvbQx3IgUCeJP943A__&Key-Pair-Id=KJLH8B0YWU4Y8M [following]\n",
      "--2026-01-23 00:52:56--  https://us.gcp.cdn.hf.co/xet-bridge-us/65778ac662d3ac1817cc9201/865f5e4682dddb29c2e20270b2471a7590c83a414bbf1d72cf4c08fdff2eeca4?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27mistral-7b-instruct-v0.2.Q4_K_M.gguf%3B+filename%3D%22mistral-7b-instruct-v0.2.Q4_K_M.gguf%22%3B&Expires=1769133176&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiRXBvY2hUaW1lIjoxNzY5MTMzMTc2fX0sIlJlc291cmNlIjoiaHR0cHM6Ly91cy5nY3AuY2RuLmhmLmNvL3hldC1icmlkZ2UtdXMvNjU3NzhhYzY2MmQzYWMxODE3Y2M5MjAxLzg2NWY1ZTQ2ODJkZGRiMjljMmUyMDI3MGIyNDcxYTc1OTBjODNhNDE0YmJmMWQ3MmNmNGMwOGZkZmYyZWVjYTRcXD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=iKdv%7EZld0Lqy96gvhfVnAcELXhcZ7wiza5QdCcN1giyJxEfZqBxvTBs6bpox4E7HPx%7EP9EZfgrqbVVn5GnwBHaYUrJk302xA0VsIvO4gBGymY9gXmOV0ptjyhv1BdjFVBFy%7EE8cG8KwYw-dsTg5hedt2433yHOXACgAJh-oNCHUCzqOqLbbu5l7s7JN7Jsd0q410Ik848pTALvKLVmO24liLv7Z79Rx3YnfRr878UjV0nYF2ouytIsc9ktLfbZQzTGz3d%7EEYeLtKX38KzBQFXvdk%7ELnkzxP45D2GvyF690U4T2nSGk94jqW42nbDmr06RxsxzlvbQx3IgUCeJP943A__&Key-Pair-Id=KJLH8B0YWU4Y8M\n",
      "Resolving us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)... 34.120.165.110\n",
      "Connecting to us.gcp.cdn.hf.co (us.gcp.cdn.hf.co)|34.120.165.110|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 4368439584 (4.1G) [application/octet-stream]\n",
      "Saving to: ‚Äò/content/models/mistral-7b-instruct.Q4_K_M.gguf‚Äô\n",
      "\n",
      "/content/models/mis 100%[===================>]   4.07G  72.9MB/s    in 47s     \n",
      "\n",
      "2026-01-23 00:53:43 (88.8 MB/s) - ‚Äò/content/models/mistral-7b-instruct.Q4_K_M.gguf‚Äô saved [4368439584/4368439584]\n",
      "\n",
      "total 4.1G\n",
      "-rw-r--r-- 1 root root 4.1G Jan 23 00:53 mistral-7b-instruct.Q4_K_M.gguf\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "MODELS_DIR = Path(\"/content/models\")\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "MODEL_PATH = MODELS_DIR / \"mistral-7b-instruct.Q4_K_M.gguf\"\n",
    "if not MODEL_PATH.exists():\n",
    "    !wget -O /content/models/mistral-7b-instruct.Q4_K_M.gguf \\\n",
    "      \"https://huggingface.co/TheBloke/Mistral-7B-Instruct-v0.2-GGUF/resolve/main/mistral-7b-instruct-v0.2.Q4_K_M.gguf?download=1\"\n",
    "\n",
    "!ls -lh /content/models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LIYvjR7bb136"
   },
   "source": [
    "## ‚öôÔ∏è Imports & Runtime Config\n",
    "\n",
    "1. Centralizes constants (context window, max tokens, temperature, stop tokens, GPU offload).\n",
    "2. Loads the `SentenceTransformer` (embeddings on GPU) and the Llama model (mistral GGUF) for generation.\n",
    "3. Provides `llm_generate()` as a thin wrapper function around `llama.cpp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510,
     "referenced_widgets": [
      "9713c43cd54c402a9b0708770647b6b6",
      "44d24bc0a1804759a3bce2af2016c852",
      "4da670d28a3d46d8b14b4fb0482d46be",
      "26e3fe7f067442dc8f6f8e5c34996477",
      "c4c6181972264baab66d2822d13266de",
      "2f7dd8673a7c4659887c74c9b8f3a72f",
      "44a8b711ea714c64825659f7f333fd89",
      "84d4066196ec4f36bf444c20dd8c0338",
      "1afcb165104a441a9b8f284396f36383",
      "0e592c63a70b412f90ce2e93f370d0af",
      "90313a6743434eeeb2d9a8a3f513e5d3",
      "66ee95b2c04842afa0d1b7626b5bdda2",
      "e297b63924364d4f880d105839ee6470",
      "5f233a26840b4cd5ade531d8652eb517",
      "5c54ed76bb4d4a1e8a3779fcb98db2a6",
      "013639ab449548ecb210b02fbde9c5d9",
      "82147f120cd947e0aa0bdc0f64a378d6",
      "8c357cbfec2946aeb69a5edbceaa8db6",
      "055054148b3b4a4894ad56320fb9c907",
      "bec5f1a76fcf47bcb7da062ea5ae6607",
      "5a2b5ffd6090421ab73380c223f4b0d5",
      "4d21c2f2fe6943a9a54da58e2a2a094a",
      "2a97b5aece734985bab0dc29eff04ff1",
      "aaf78db743374d57b8e9cde9cfa1c132",
      "388e8f5c9e9546b5ac769b9ab312e8fb",
      "22f4965337e64342928e5b723ab6da15",
      "8e7e12dbd93b49778a0be54596f30b8d",
      "aa4d44be21c24842b909e9efc14b9b0d",
      "92e705f733974763aafffb095b8243c6",
      "696091930f524ba1a93665604faaf9e9",
      "2b8beb9f4ea148cea2ea94a2b3ef5579",
      "e74608cbd58a4990a2a594584ecd3d9d",
      "97779505c3ef42108f9fcfd8c913d83f",
      "11706eb5fc304663b12504013d12968c",
      "133cd4745a1b478cb66636a30eba766f",
      "cae9f6efa21241189118c6aa1d88b145",
      "12656dac10f747bb935301a1c48b1b58",
      "d1dbe851d621424c9ee14e2d6a8f5002",
      "2c8d207fd33e475c8e3061e299e2f806",
      "a2130b2ac97e42c1bf47ff18ca6c0f43",
      "5a23a34955614eb387a4c10e7c8dadc6",
      "e62716357f074f5eb38066e485014e42",
      "fbf09ebf0e384351ad26c8a1f3133ee8",
      "ec93a6e4760e421f9302b46cb2c2d246",
      "376b6500ef6540d395b281db3c576780",
      "df6d020444f547708e3328baeb507c07",
      "257edfaeefd7410c849460af1e79829e",
      "2e43627121c54db59a068e83689cda13",
      "0fc1e58a257f4baa908a4af956b80eac",
      "73e3bbe585084838b29f929f94d7922b",
      "ff10d491f2fe447d8b9e77a1cf044463",
      "eb886086300b4b4f99ab3d503c070f09",
      "79b17cbe6f0648479c57f6872e7acb7c",
      "4fcb5ad287f04b00acbd06abd2e9eea9",
      "30d88d57d1064d60aec6164faf29d87f",
      "1900d16e043f401f8c40d4e5e8dc557b",
      "e22f294063e84a6d95192e9441e5ad0e",
      "3e59b80f8afc422d9f8d34731667e051",
      "a4da0891034649a5a946568ced9af4dc",
      "4e8b5b08af0942a896a26313eef12cd4",
      "31cfebf401c7494782eca9c9bc253055",
      "2fcccf1348f74d39adb917127cb3ddb9",
      "baba3aa9ddce493d9bbeaa661b80af78",
      "4a586de99ffe4b7bb457d8842030d45d",
      "05b976eae8d8467ab81bfee905cb0e40",
      "185cb6fe04234c8ba91ae53960402d1a",
      "6ca2d00b0d01424eba0934f7c3c4391f",
      "016f12d4619b4d6fab44dcfb8f7720f4",
      "9664a6a10aaa4b25b3179752fd71a107",
      "e3152d05882740a399146eb35529c030",
      "f49afadce67c48e5997570cc409f943b",
      "f4abab2b30914a718dfe48952b616bd8",
      "716dfdeb40954d4baa8ee37f5ecaead0",
      "9495e1b2070344658240c9a825a75a5d",
      "e02e0f7456aa40de8e6f7c44359f8adc",
      "3c7fdfecbb4145d89447bb51986feb42",
      "f1dbe9cdabf54901ae9034e98c0d862d",
      "38a3abf2582f4a1c9bc544dd04341b68",
      "3b70bfb6120c441abfe60e1f5847262d",
      "b58092f8016c4b0e882bb5bf02743af1",
      "df65582a0b5f4575899651d41ac86928",
      "2bc2c703b7d8482ab1332fab39ed9aa1",
      "de261bbd9d3f45eb91302f79720094eb",
      "7807b3680fc140ef8c78c9fc0a803fb3",
      "79eb8115e0c144aea56e39b4a22719f1",
      "a64ce997347b4a1495d0be202f732506",
      "3d75f2b592b1458e93d3438fc644d019",
      "96e082e8fae34a26b3a866213fe44e78",
      "b4d30ef5cf614b6da1ff495f6ed8a8c7",
      "29550fa75ebe41aba5914b98a129d539",
      "7bf7909823e44ab8a2c46119d9437fb3",
      "14373edb77834fecadcbc59e4bba59c2",
      "3cce1812021f4dda982cc78e62bbdca5",
      "65773b31e28448518055e137610b3f3f",
      "1d9cccd7afac485aa1d9d95b424d119e",
      "0ef35f4c729f4e6b9c1366d102110484",
      "23a72d545ec24d95acab09d77fa98cd7",
      "0ed0dca696944c569a517cb3723d8003",
      "0bee221de5c34701aaa86efff7be8beb",
      "4a5044335095462b92aa36a111ca0f6b",
      "30bbedfa152a448dbc6a2e57ac11f011",
      "935ad739f5c64b8bbbed63e51266f249",
      "8d51ec8723c74cc1929f39a8e3c770ac",
      "e9be6a686be14013add12994ab23f31b",
      "478acc4ed6f6468cb7e59463be6effc4",
      "ca570576af6042e3affde79373539b2c",
      "79cc9a7e81574d1ebbbdfba9810837fb",
      "38427524056b4c84be229c2f0394db11",
      "bf1fef2c5b71493db87628d8376d76ee",
      "3223fcc9025a48c795bc1b5dc145095c",
      "cbcb1c83b7de410c9195060bc8d9da9f",
      "bb6036b28f714f4aa1953d4458e912e2",
      "44fe85f9268f459381bfbe6cbca86444",
      "5dff70effbde40c5962c6f3a2301fd78",
      "1c5acff344364047a4526934fff9692e",
      "024b60e912124de49f62c4fb32aba4a6",
      "8fb689f387c74a2f81dbd25fb7df8540",
      "34f1e6df1809434b94b3ba1f67379884",
      "8d9f9976e35648dfab35732baf51df40",
      "fc8733afabb2442ba45529d8f982cc48",
      "cf20414d0255425fa028d97127ce0ca6"
     ]
    },
    "id": "cjjNiQeZb3EF",
    "outputId": "d8a30c3a-e1a0-41e8-d40a-477ad93313b5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9713c43cd54c402a9b0708770647b6b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66ee95b2c04842afa0d1b7626b5bdda2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a97b5aece734985bab0dc29eff04ff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11706eb5fc304663b12504013d12968c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "376b6500ef6540d395b281db3c576780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1900d16e043f401f8c40d4e5e8dc557b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ca2d00b0d01424eba0934f7c3c4391f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38a3abf2582f4a1c9bc544dd04341b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4d30ef5cf614b6da1ff495f6ed8a8c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a5044335095462b92aa36a111ca0f6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcb1c83b7de410c9195060bc8d9da9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (4096) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
     ]
    }
   ],
   "source": [
    "import os, io, json\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfReader\n",
    "\n",
    "import gradio as gr\n",
    "from gradio_pdf import PDF\n",
    "\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "# -----------------------------\n",
    "# Model paths & knobs\n",
    "# -----------------------------\n",
    "MISTRAL_GGUF_PATH = \"/content/models/mistral-7b-instruct.Q4_K_M.gguf\"\n",
    "LLM_CTX         = 4096\n",
    "LLM_MAX_TOKENS  = 600\n",
    "LLM_TEMP        = 0.1\n",
    "LLM_STOP        = [\"</s>\"]\n",
    "\n",
    "# Use -1 / 999 to offload all layers to GPU (when possible)\n",
    "# N_GPU_LAYERS    = -1\n",
    "N_GPU_LAYERS    = 999\n",
    "\n",
    "# -----------------------------\n",
    "# Embeddings on GPU\n",
    "# -----------------------------\n",
    "EMBEDDING_MODEL_ID = \"all-MiniLM-L6-v2\"\n",
    "embed_model = SentenceTransformer(EMBEDDING_MODEL_ID, device=\"cuda\")\n",
    "\n",
    "# -----------------------------\n",
    "# Load local Mistral (GPU offload)\n",
    "# -----------------------------\n",
    "assert os.path.exists(MISTRAL_GGUF_PATH), f\"Missing GGUF at {MISTRAL_GGUF_PATH}\"\n",
    "llm = Llama(\n",
    "    model_path=MISTRAL_GGUF_PATH,\n",
    "    n_ctx=LLM_CTX,\n",
    "    n_gpu_layers=N_GPU_LAYERS,\n",
    "    verbose=False  # set True once to see CUDA offload details\n",
    ")\n",
    "\n",
    "def llm_generate(prompt: str, max_tokens: int = LLM_MAX_TOKENS, temperature: float = LLM_TEMP,\n",
    "                 stop: Optional[List[str]] = None) -> str:\n",
    "    out = llm(\n",
    "        prompt=prompt,\n",
    "        max_tokens=max_tokens,\n",
    "        temperature=temperature,\n",
    "        stop=stop or LLM_STOP,\n",
    "        echo=False\n",
    "    )\n",
    "    return (out.get(\"choices\", [{}])[0].get(\"text\") or \"\").strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57TydLVAb6h5"
   },
   "source": [
    "## üß± Data Structures\n",
    "\n",
    "Lightweight `dataclass` models used throughout:\n",
    "*   `PageInfo` (raw page text + doc hints)\n",
    "*   `LogicalDocument` (grouped pages = one logical doc)\n",
    "*   `ChunkMetadata` (retrieval units with page range + embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "zWLewm1Tb73E"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageInfo:\n",
    "    page_num: int\n",
    "    text: str\n",
    "    doc_type: Optional[str] = None\n",
    "    page_in_doc: int = 0\n",
    "\n",
    "@dataclass\n",
    "class LogicalDocument:\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    chunks: List[Dict] = None\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    chunk_index: int\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    embedding: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYluN7Gm2qF5"
   },
   "source": [
    "## üè∑Ô∏è Rule-Based Doc Typing Keywords\n",
    "\n",
    "1. Keyword tags (literal classification) for common document types (Pay Slip, Contract, Lender Fee Sheet, Invoice, etc.).\n",
    "2. Enables fast, LLM-free classification during document ingestion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "22KZILEHYKeu"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# ---- 1) Ultra-simple keyword bags (lowercase) ----\n",
    "DOC_KEYWORDS = {\n",
    "    \"Resume\": [\n",
    "        \"experience\",\"education\",\"skills\",\"projects\",\"summary\",\"objective\",\n",
    "        \"work history\",\"employment history\"\n",
    "        ],\n",
    "\n",
    "    \"Contract\": [\n",
    "        \"agreement\",\"terms\",\"parties\",\"obligations\",\"hereby\",\"whereas\",\n",
    "        \"governing law\",\"termination\",\"confidentiality\"\n",
    "        ],\n",
    "\n",
    "    \"Mortgage Contract\": [\n",
    "        \"mortgage\",\"deed of trust\",\"note\",\"borrower\",\"lender\",\n",
    "        \"property address\",\"interest rate\",\"escrow\",\"principal and interest\"\n",
    "        ],\n",
    "\n",
    "    \"Invoice\": [\n",
    "        \"invoice\",\"invoice no\",\"invoice number\",\"bill to\",\"ship to\",\n",
    "        \"unit price\",\"qty\",\"subtotal\",\"balance due\",\"payment terms\"\n",
    "        ],\n",
    "\n",
    "    \"Pay Slip\": [\n",
    "        \"salary\",\"wages\",\"pay period\",\"gross pay\",\"gross income\",\"net pay\",\n",
    "        \"earnings\",\"deductions\",\"withholding\",\"ytd\",\"pay date\",\"employee\"\n",
    "        ],\n",
    "\n",
    "    \"Lender Fee Sheet\": [\n",
    "        \"fee worksheet\",\"lender fee\",\"origination fees\",\"origination charges\",\n",
    "        \"closing costs\",\"underwriting fee\",\"processing fee\",\"appraisal fee\",\n",
    "        \"credit report\",\"escrow\",\"title\"\n",
    "        ],\n",
    "\n",
    "    \"Land Deed\": [\n",
    "        \"deed\",\"warranty deed\",\"quitclaim deed\",\"grantor\",\"grantee\",\n",
    "        \"legal description\",\"parcel\",\"notary\",\"recorded\",\"county\"\n",
    "        ],\n",
    "\n",
    "    \"Bank Statement\": [\n",
    "        \"statement\",\"account number\",\"account ending in\",\"ending balance\",\n",
    "        \"available balance\",\"transactions\",\"deposits\",\"withdrawals\",\n",
    "        \"statement period\"\n",
    "        ],\n",
    "\n",
    "    \"Tax Document\": [\n",
    "        \"tax\",\"withholding\",\"refund\",\"filing status\",\"form w-2\",\"w-2\",\n",
    "        \"1099\",\"1040\",\"internal revenue service\",\"department of the treasury\"\n",
    "        ],\n",
    "\n",
    "    \"Insurance\": [\n",
    "        \"policy\",\"coverage\",\"premium\",\"claim\",\"policy number\",\n",
    "        \"effective date\",\"expiration date\",\"deductible\",\"limits\"\n",
    "        ],\n",
    "\n",
    "    \"Report\": [\n",
    "        \"abstract\",\"executive summary\",\"methodology\",\"analysis\",\"findings\",\n",
    "        \"results\",\"conclusion\",\"discussion\",\"recommendations\"\n",
    "        ],\n",
    "\n",
    "    \"Letter\": [\n",
    "        \"dear \",\"sincerely\",\"regards\",\"to whom it may concern\",\"subject:\"\n",
    "        ],\n",
    "\n",
    "    \"Form\": [\n",
    "        \"form\",\"application\",\"applicant\",\"fields\",\"signature\",\"instructions\",\n",
    "        \"please print\",\"submit\"\n",
    "        ],\n",
    "\n",
    "    \"ID Document\": [\n",
    "        \"date of birth\",\"issue date\",\"expiration date\",\"driver's license\",\n",
    "        \"id number\",\"passport\",\"national id\"\n",
    "        ],\n",
    "\n",
    "    \"Medical\": [\n",
    "        \"prescription\",\"dosage\",\"diagnosis\",\"patient\",\"provider\",\n",
    "        \"medical record\",\"mrn\",\"icd-10\",\"cpt\",\"visit date\",\"discharge\"\n",
    "        ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "t8Lc0_itfbMi"
   },
   "outputs": [],
   "source": [
    "# ---- 2) Core helpers ----\n",
    "def _norm_text(text: str) -> str:\n",
    "    return (text or \"\").lower()\n",
    "\n",
    "def classify_simple(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Count keyword hits per label; return label with max hits.\n",
    "    Ties: first seen. Zero hits: 'Other'.\n",
    "    \"\"\"\n",
    "    t = _norm_text(text)\n",
    "    best_label, best_hits = \"Other\", 0\n",
    "    for label, words in DOC_KEYWORDS.items():\n",
    "        hits = sum(1 for w in words if w in t)\n",
    "        if hits > best_hits:\n",
    "            best_label, best_hits = label, hits\n",
    "    return best_label if best_hits > 0 else \"Other\"\n",
    "\n",
    "PAGE_OF_RE = re.compile(r\"\\bpage\\s+(\\d+)\\s+of\\s+(\\d+)\\b\", re.I)\n",
    "\n",
    "def has_continuous_page_number(prev_text: str, curr_text: str) -> bool:\n",
    "    \"\"\"True if 'Page i of N' -> 'Page i+1 of N'.\"\"\"\n",
    "    p = PAGE_OF_RE.search(_norm_text(prev_text))\n",
    "    c = PAGE_OF_RE.search(_norm_text(curr_text))\n",
    "    if not (p and c):\n",
    "        return False\n",
    "    try:\n",
    "        return int(c.group(1)) == int(p.group(1)) + 1 and c.group(2) == p.group(2)\n",
    "    except:\n",
    "        return False\n",
    "\n",
    "def same_doc(prev_label: str, curr_label: str) -> bool:\n",
    "    \"\"\"Boundary heuristic: same predicted label ‚áí same logical document.\"\"\"\n",
    "    return (prev_label or \"Other\") == (curr_label or \"Other\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMqRJzJt3GZI"
   },
   "source": [
    "## üîÄ Boundary/Type Detection Switches\n",
    "\n",
    "Feature flags:\n",
    "- `USE_RULES_FOR_UPLOAD`: use keyword rules at upload time (skip LLM).\n",
    "- `USE_SIMPLE_QUERY_ROUTER`: optional keyword router at query time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vZIZLPfrfjpF"
   },
   "outputs": [],
   "source": [
    "# ---- 3) Switches: use rules during upload; keep LLM for answers ----\n",
    "USE_RULES_FOR_UPLOAD = True          # <‚Äî keep True to avoid LLM during ingestion\n",
    "USE_SIMPLE_QUERY_ROUTER = False      # <‚Äî set True to avoid LLM at query-routing time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h6D8gUPy3Q2R"
   },
   "source": [
    "## üß≠ Routing Helper Functions (Upload & Query)\n",
    "\n",
    "1. `classify_document_type()` ‚Üí follows key-word based rules (or fallback to any LLM version if you flip the flag).\n",
    "2. `detect_document_boundary()` ‚Üí keeps multi-page docs together (page-number continuity + label consistency).\n",
    "3. Optional `predict_query_document_type()` if you enable the simple router."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "jF00SDRsfm9b"
   },
   "outputs": [],
   "source": [
    "# ---- 4) Patch the names your ingestion loop already calls ----\n",
    "\n",
    "def classify_document_type(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Upload-time classifier.\n",
    "    If USE_RULES_FOR_UPLOAD is True, use simple keyword rules (fast).\n",
    "    Otherwise, fall back to any previously defined LLM-based version (if available).\n",
    "    \"\"\"\n",
    "    if USE_RULES_FOR_UPLOAD:\n",
    "        return classify_simple(text)\n",
    "    # Fallback path if you ever flip the switch off:\n",
    "    try:\n",
    "        return llm_classify_document_type(text)  # only if you kept an LLM impl around under this name\n",
    "    except NameError:\n",
    "        return classify_simple(text)\n",
    "\n",
    "def detect_document_boundary(prev_text: str, curr_text: str, current_doc_type: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Upload-time boundary detector.\n",
    "    - First, honor page-number continuity (Page i of N -> Page i+1 of N).\n",
    "    - Else, same-label heuristic using the simple classifier.\n",
    "    Return True if 'curr_text' continues the same logical doc; False if a new doc starts.\n",
    "    \"\"\"\n",
    "    if has_continuous_page_number(prev_text, curr_text):\n",
    "        return True\n",
    "    prev_label = current_doc_type or \"Other\"\n",
    "    curr_label = classify_simple(curr_text) if USE_RULES_FOR_UPLOAD else classify_document_type(curr_text)\n",
    "    return same_doc(prev_label, curr_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "Dzq5XBsHfqFk"
   },
   "outputs": [],
   "source": [
    "# ---- 5) Optional: lightweight query-time router to avoid LLM routing ----\n",
    "def predict_query_document_type(query: str):\n",
    "    \"\"\"\n",
    "    Optional, non-LLM query router that reuses the same keyword bags.\n",
    "    Returns (label, confidence[0..1]).\n",
    "    \"\"\"\n",
    "    if not USE_SIMPLE_QUERY_ROUTER:\n",
    "        # If you still have an LLM router elsewhere, your app can call that instead.\n",
    "        # Returning (\"Other\", 0.0) effectively disables routing.\n",
    "        return \"Other\", 0.0\n",
    "\n",
    "    t = _norm_text(query)\n",
    "    best, hits = \"Other\", 0\n",
    "    for label, words in DOC_KEYWORDS.items():\n",
    "        h = sum(1 for w in words if w in t)\n",
    "        if h > hits:\n",
    "            best, hits = label, h\n",
    "    conf = 0.9 if hits >= 2 else (0.6 if hits == 1 else 0.0)\n",
    "    return best, conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxGrYeJ13iNP"
   },
   "source": [
    "## üìë PDF Extraction + OCR + Logical Segmentation\n",
    "\n",
    "1. Opens PDF via PyMuPDF, extracts text; if empty, runs Tesseract OCR on page image.\n",
    "2. Groups pages into logical documents using the rule-based boundary detector.\n",
    "3. Returns (`pages_info`, `logical_docs`) for downstream chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "UmBhvvBWmobE"
   },
   "outputs": [],
   "source": [
    "# --- PDF extraction + OCR + logical segmentation (no LLM) ---\n",
    "def extract_and_analyze_pdf(pdf_file):\n",
    "    # open\n",
    "    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n",
    "        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n",
    "    elif hasattr(pdf_file, \"read\"):\n",
    "        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
    "    else:\n",
    "        doc = fitz.open(pdf_file)\n",
    "\n",
    "    pages_info = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text() or \"\"\n",
    "        if not text.strip():\n",
    "            try:\n",
    "                pix = page.get_pixmap()\n",
    "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                text = pytesseract.image_to_string(img) or \"\"\n",
    "            except Exception:\n",
    "                text = \"\"\n",
    "        pi = PageInfo(page_num=i, text=text)\n",
    "        pages_info.append(pi)\n",
    "    doc.close()\n",
    "\n",
    "    if not pages_info:\n",
    "        raise ValueError(\"No text extracted from PDF\")\n",
    "\n",
    "    logical_docs, current_doc_pages = [], []\n",
    "    current_doc_type, doc_counter = None, 0\n",
    "\n",
    "    for i, page_info in enumerate(pages_info):\n",
    "        if i == 0:\n",
    "            current_doc_type = classify_document_type(page_info.text)   # <- rules\n",
    "            page_info.doc_type = current_doc_type\n",
    "            page_info.page_in_doc = 0\n",
    "            current_doc_pages = [page_info]\n",
    "        else:\n",
    "            same = detect_document_boundary(pages_info[i-1].text, page_info.text, current_doc_type)  # <- rules\n",
    "            if same:\n",
    "                page_info.doc_type = current_doc_type\n",
    "                page_info.page_in_doc = len(current_doc_pages)\n",
    "                current_doc_pages.append(page_info)\n",
    "            else:\n",
    "                logical_docs.append(LogicalDocument(\n",
    "                    doc_id=f\"doc_{doc_counter}\",\n",
    "                    doc_type=current_doc_type,\n",
    "                    page_start=current_doc_pages[0].page_num,\n",
    "                    page_end=current_doc_pages[-1].page_num,\n",
    "                    text=\"\\n\\n\".join(p.text for p in current_doc_pages)\n",
    "                ))\n",
    "                doc_counter += 1\n",
    "                current_doc_type = classify_document_type(page_info.text)  # <- rules\n",
    "                page_info.doc_type = current_doc_type\n",
    "                page_info.page_in_doc = 0\n",
    "                current_doc_pages = [page_info]\n",
    "\n",
    "    if current_doc_pages:\n",
    "        logical_docs.append(LogicalDocument(\n",
    "            doc_id=f\"doc_{doc_counter}\",\n",
    "            doc_type=current_doc_type,\n",
    "            page_start=current_doc_pages[0].page_num,\n",
    "            page_end=current_doc_pages[-1].page_num,\n",
    "            text=\"\\n\\n\".join(p.text for p in current_doc_pages)\n",
    "        ))\n",
    "\n",
    "    return pages_info, logical_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1rjEvKnD3qXi"
   },
   "source": [
    "## ‚úÇÔ∏è Chunking with Page Ranges\n",
    "\n",
    "1. `chunk_document_with_metadata()` splits each logical document into overlapping text windows. Chunk size of 500 tokens and an overlap of 100 tokens are taken.\n",
    "      - Every chunk carries doc_type + estimated page_start/end for later source citing.\n",
    "2. `process_all_documents()` applies chunking across the packet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ugm3biRFmqdh"
   },
   "outputs": [],
   "source": [
    "# --- Chunking with metadata (no LLM) ---\n",
    "def chunk_document_with_metadata(logical_doc: LogicalDocument, chunk_size: int = 500, overlap: int = 100):\n",
    "    chunks, words = [], logical_doc.text.split()\n",
    "    if len(words) <= chunk_size:\n",
    "        return [ChunkMetadata(\n",
    "            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n",
    "            doc_id=logical_doc.doc_id,\n",
    "            doc_type=logical_doc.doc_type,\n",
    "            chunk_index=0,\n",
    "            page_start=logical_doc.page_start,\n",
    "            page_end=logical_doc.page_end,\n",
    "            text=logical_doc.text\n",
    "        )]\n",
    "    stride, i = max(1, chunk_size - overlap), 0\n",
    "    while i < len(words):\n",
    "        end = min(i + chunk_size, len(words))\n",
    "        chunk_text = \" \".join(words[i:end])\n",
    "        rel = i / max(1, len(words))\n",
    "        page_range = max(1, logical_doc.page_end - logical_doc.page_start + 1)\n",
    "        chunk_page_start = logical_doc.page_start + int(rel * page_range)\n",
    "        chunk_page_end = min(logical_doc.page_end, chunk_page_start + 1)\n",
    "        chunks.append(ChunkMetadata(\n",
    "            chunk_id=f\"{logical_doc.doc_id}_chunk_{len(chunks)}\",\n",
    "            doc_id=logical_doc.doc_id,\n",
    "            doc_type=logical_doc.doc_type,\n",
    "            chunk_index=len(chunks),\n",
    "            page_start=chunk_page_start,\n",
    "            page_end=chunk_page_end,\n",
    "            text=chunk_text\n",
    "        ))\n",
    "        if end >= len(words): break\n",
    "        i += stride\n",
    "    return chunks\n",
    "\n",
    "def process_all_documents(logical_docs: List[LogicalDocument]) -> List[ChunkMetadata]:\n",
    "    all_chunks = []\n",
    "    for ld in logical_docs:\n",
    "        cks = chunk_document_with_metadata(ld, chunk_size=500, overlap=100)\n",
    "        # ensure doc_type carried into each chunk\n",
    "        for c in cks:\n",
    "            c.doc_type = ld.doc_type\n",
    "        ld.chunks = cks\n",
    "        all_chunks.extend(cks)\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3lPIXsAS34Ws"
   },
   "source": [
    "## üîé Retriever (FAISS) + Per-Type Indices\n",
    "\n",
    "1. Builds a global FAISS index over all chunk embeddings.\n",
    "2. Also builds per-doc-type sub-indices for filtered retrieval (e.g., only ‚ÄúLender Fee Sheet‚Äù).\n",
    "3. `retrieve()` returns top-k chunks with normalized relevance scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "PfBEWqF-mu6f"
   },
   "outputs": [],
   "source": [
    "# --- Retriever & FAISS indices (no LLM router) ---\n",
    "class IntelligentRetriever:\n",
    "    def __init__(self):\n",
    "        self.index = None\n",
    "        self.chunks: List[ChunkMetadata] = []\n",
    "        self.doc_type_indices: Dict[str, Dict] = {}\n",
    "\n",
    "    def build_indices(self, chunks: List[ChunkMetadata]):\n",
    "        self.chunks = chunks\n",
    "        texts = [c.text for c in chunks]\n",
    "        embs = embed_model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "        for i, c in enumerate(chunks):\n",
    "            c.embedding = embs[i]\n",
    "        dim = embs.shape[1]\n",
    "        self.index = faiss.IndexFlatL2(dim)\n",
    "        self.index.add(embs)\n",
    "\n",
    "        self.doc_type_indices = {}\n",
    "        for dt in sorted(set(c.doc_type for c in chunks)):\n",
    "            idxs = [i for i, c in enumerate(chunks) if c.doc_type == dt]\n",
    "            if not idxs:\n",
    "                continue\n",
    "            sub = faiss.IndexFlatL2(dim)\n",
    "            sub.add(embs[idxs])\n",
    "            self.doc_type_indices[dt] = {\"index\": sub, \"map\": idxs}\n",
    "\n",
    "    def retrieve(self, query: str, k: int = 4, filter_doc_type: Optional[str] = None, auto_route: bool = False):\n",
    "        q = embed_model.encode([query], convert_to_numpy=True)\n",
    "        # no LLM routing; use filter when provided, else global\n",
    "        if filter_doc_type and filter_doc_type in self.doc_type_indices:\n",
    "            td = self.doc_type_indices[filter_doc_type]\n",
    "            D, I = td[\"index\"].search(q, k)\n",
    "            ids, dists = [td[\"map\"][i] for i in I[0]], D[0]\n",
    "        else:\n",
    "            D, I = self.index.search(q, k)\n",
    "            ids, dists = I[0], D[0]\n",
    "\n",
    "        maxd = float(max(dists) if len(dists) else 1.0)\n",
    "        scores = [(maxd - d) / maxd for d in dists]\n",
    "        return [(self.chunks[i], scores[j]) for j, i in enumerate(ids)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_-1VVST4AsX"
   },
   "source": [
    "## üß† Grounded Answer Assembly (LLM)\n",
    "\n",
    "1. `answer_with_sources`: End-to-end coordinator for building a safe, compact context and generating an answer strictly from retrieved chunks.\n",
    "2. `summarize_documents_llm`: LLM summary across logical documents (Pay Slip, Contract, Lender Fee Sheet, etc.), grouped for quick scanning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "BUq6I0B1nbPv"
   },
   "outputs": [],
   "source": [
    "# --- Replacement: grounded answer with token budget + de-dupe ---\n",
    "\n",
    "# If you already set these elsewhere, keep them; otherwise these defaults are safe.\n",
    "LLM_CTX = 4096                 # Mistral-7B context window\n",
    "LLM_MAX_OUT = min(LLM_MAX_TOKENS, 512) if \"LLM_MAX_TOKENS\" in globals() else 512\n",
    "LLM_TEMP = globals().get(\"LLM_TEMP\", 0.2)\n",
    "\n",
    "def _approx_token_count(s: str) -> int:\n",
    "    # ~4 chars per token for English; good enough to avoid overflows.\n",
    "    return max(1, len(s) // 4)\n",
    "\n",
    "def _shrink_to_budget(blocks: list[str], max_ctx_tokens: int) -> str:\n",
    "    \"\"\"Concatenate blocks until ~max_ctx_tokens; trim last block if needed.\"\"\"\n",
    "    out, used = [], 0\n",
    "    for b in blocks:\n",
    "        t = _approx_token_count(b)\n",
    "        if used + t > max_ctx_tokens:\n",
    "            need = max_ctx_tokens - used\n",
    "            out.append(b[:need * 4])  # approx chars\n",
    "            break\n",
    "        out.append(b)\n",
    "        used += t\n",
    "    return \"\\n\".join(out)\n",
    "\n",
    "def answer_with_sources(question: str, retrieved: List[Tuple[ChunkMetadata, float]]) -> Dict:\n",
    "    \"\"\"\n",
    "    Build a compact, de-duped context from retrieved chunks and ask the LLM to answer.\n",
    "    Returns {answer, sources[], confidence}.\n",
    "    \"\"\"\n",
    "    if not retrieved:\n",
    "        return {\"answer\": \"I couldn't find enough context in the document(s).\",\n",
    "                \"sources\": [], \"confidence\": 0.0}\n",
    "\n",
    "    # 1) De-dupe by chunk_id (preserve order)\n",
    "    unique, seen = [], set()\n",
    "    for cm, sc in retrieved:\n",
    "        if getattr(cm, \"chunk_id\", None) in seen:\n",
    "            continue\n",
    "        seen.add(getattr(cm, \"chunk_id\", id(cm)))\n",
    "        unique.append((cm, sc))\n",
    "\n",
    "    # 2) Build labeled blocks + sources\n",
    "    blocks, sources = [], []\n",
    "    for cm, sc in unique:\n",
    "        header = f\"[From {cm.doc_type}, Pages {cm.page_start}-{cm.page_end}, score {sc:.2f}]\"\n",
    "        blocks.append(header + \"\\n\" + cm.text)\n",
    "        sources.append({\n",
    "            \"doc_type\": cm.doc_type,\n",
    "            \"pages\": f\"{cm.page_start}-{cm.page_end}\",\n",
    "            \"relevance\": f\"{sc:.2%}\",\n",
    "            \"preview\": (cm.text[:160] + \"...\") if len(cm.text) > 160 else cm.text\n",
    "        })\n",
    "\n",
    "    # 3) Respect the model's context window: leave headroom for the prompt + reply\n",
    "    #    (prompt scaffolding ~200‚Äì300 tokens; adjust margin if you change formatting)\n",
    "    PROMPT_MARGIN = 256\n",
    "    ctx_budget = max(256, LLM_CTX - LLM_MAX_OUT - PROMPT_MARGIN)\n",
    "    context = _shrink_to_budget(blocks, max_ctx_tokens=ctx_budget)\n",
    "\n",
    "    prompt = f\"\"\"Use ONLY the context to answer. If the context is insufficient, say you don't know.\n",
    "Cite doc types and pages briefly.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    "\n",
    "    try:\n",
    "        txt = llm_generate(prompt, max_tokens=LLM_MAX_OUT, temperature=LLM_TEMP)\n",
    "        avg_conf = float(sum(sc for _, sc in unique) / max(1, len(unique)))\n",
    "        return {\"answer\": (txt or \"\").strip(), \"sources\": sources, \"confidence\": avg_conf}\n",
    "    except Exception as e:\n",
    "        return {\"answer\": f\"Error generating answer: {e}\", \"sources\": sources, \"confidence\": 0.0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gehRL4i-dRDX"
   },
   "outputs": [],
   "source": [
    "# ---------- Shared helper functions ----------\n",
    "def _approx_token_count(s: str) -> int:\n",
    "    return max(1, len(s) // 4)\n",
    "\n",
    "def _shrink_to_budget(blocks: list[str], max_ctx_tokens: int) -> str:\n",
    "    out, used = [], 0\n",
    "    for b in blocks:\n",
    "        t = _approx_token_count(b)\n",
    "        if used + t > max_ctx_tokens:\n",
    "            need = max_ctx_tokens - used\n",
    "            out.append(b[:need * 4])\n",
    "            break\n",
    "        out.append(b)\n",
    "        used += t\n",
    "    return \"\\n\\n\".join(out)\n",
    "\n",
    "# ---------- Summary (LLM) ----------\n",
    "def summarize_documents_llm(docs: List[LogicalDocument], max_ctx_tokens: int = None) -> str:\n",
    "    if not docs:\n",
    "        return \"No documents loaded.\"\n",
    "    # Build lightweight context: first ~800 chars of each doc with labels & pages\n",
    "    blocks = []\n",
    "    for d in docs:\n",
    "        hdr = f\"[{d.doc_type} | Pages {d.page_start+1}-{d.page_end+1}]\"\n",
    "        txt = (d.text[:1200] + \"...\") if len(d.text) > 1200 else d.text\n",
    "        blocks.append(hdr + \"\\n\" + txt)\n",
    "\n",
    "    # Respect the model window\n",
    "    ctx_budget = max_ctx_tokens or max(256, LLM_CTX - 512)  # reserve ~512 for instructions + answer\n",
    "    context = _shrink_to_budget(blocks, ctx_budget)\n",
    "\n",
    "    prompt = f\"\"\"Summarize the following multi-document packet for a non-expert.\n",
    "Group bullets by *document type* (Pay Slip, Contract, Lender Fee Sheet, etc.).\n",
    "Keep it concise and strictly grounded in the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Write:\n",
    "- A 3‚Äì6 bullet overview overall\n",
    "- Then 2‚Äì4 bullets per document type found\n",
    "- If any amounts appear, reference them briefly with the page range\n",
    "\"\"\"\n",
    "    return llm_generate(prompt, max_tokens=min(LLM_MAX_TOKENS, 600), temperature=LLM_TEMP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PbhXdwaMqI9y"
   },
   "source": [
    "## üóÇÔ∏è LocalDocStore Orchestrator\n",
    "\n",
    "End-to-end coordinator for:\n",
    "- `process_pdf()`: extract ‚Üí segment ‚Üí chunk ‚Üí embed ‚Üí index\n",
    "- `query()`: retrieve ‚Üí generate grounded answer\n",
    "- `summarize()`: LLM summary across logical docs\n",
    "- `structure()`: compact view for the UI (doc type, page range, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "Rh9Rhq8KqJ0P"
   },
   "outputs": [],
   "source": [
    "class LocalDocStore:\n",
    "    def __init__(self):\n",
    "        self.pages: List[PageInfo] = []\n",
    "        self.docs: List[LogicalDocument] = []\n",
    "        self.chunks: List[ChunkMetadata] = []\n",
    "        self.retriever = IntelligentRetriever()\n",
    "        self.ready = False\n",
    "        self.stats: Dict = {}\n",
    "        self.filename: Optional[str] = None\n",
    "\n",
    "    def process_pdf(self, pdf_file, filename=\"document.pdf\"):\n",
    "        self.ready = False\n",
    "        self.filename = filename\n",
    "        t0 = datetime.now()\n",
    "        self.pages, self.docs = extract_and_analyze_pdf(pdf_file)\n",
    "        self.chunks = process_all_documents(self.docs)\n",
    "        self.retriever.build_indices(self.chunks)\n",
    "        dt = (datetime.now() - t0).total_seconds()\n",
    "        self.stats = {\n",
    "            \"filename\": filename,\n",
    "            \"pages\": len(self.pages),\n",
    "            \"docs\": len(self.docs),\n",
    "            \"chunks\": len(self.chunks),\n",
    "            \"types\": sorted(list(set(d.doc_type for d in self.docs))),\n",
    "            \"time\": f\"{dt:.1f}s\"\n",
    "        }\n",
    "        self.ready = True\n",
    "        return True, self.stats\n",
    "\n",
    "    def query(self, q: str, k: int = 4, filter_type: Optional[str] = None, auto_route: bool = True):\n",
    "        if not self.ready:\n",
    "            return {\"answer\": \"Please upload and process a PDF first.\", \"sources\": [], \"confidence\": 0.0}\n",
    "        hits = self.retriever.retrieve(q, k=k, filter_doc_type=filter_type, auto_route=auto_route)\n",
    "        return answer_with_sources(q, hits)\n",
    "\n",
    "    def structure(self) -> List[Dict]:\n",
    "        return [{\n",
    "            \"id\": d.doc_id,\n",
    "            \"type\": d.doc_type,\n",
    "            \"pages\": f\"{d.page_start + 1}-{d.page_end + 1}\",\n",
    "            \"chunks\": len(d.chunks) if d.chunks else 0\n",
    "        } for d in self.docs]\n",
    "\n",
    "\n",
    "    def summarize(self) -> Dict:\n",
    "        if not self.ready:\n",
    "            return {\"answer\": \"Please upload and process a PDF first.\", \"sources\": [], \"confidence\": None}\n",
    "        txt = summarize_documents_llm(self.docs, max_ctx_tokens=LLM_CTX - 512)\n",
    "        # Build lightweight sources = one per doc\n",
    "        sources = [{\n",
    "            \"doc_type\": d.doc_type,\n",
    "            \"pages\": f\"{d.page_start+1}-{d.page_end+1}\",\n",
    "            \"relevance\": \"\",\n",
    "            \"preview\": (d.text[:160] + \"...\") if len(d.text) > 160 else d.text\n",
    "        } for d in self.docs]\n",
    "        return {\"answer\": txt.strip(), \"sources\": sources, \"confidence\": None}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6WAwWp9hqNQU"
   },
   "source": [
    "## üß© UI Helpers (Sources + Suggestions)\n",
    "1. `_render_sources()` ‚Üí pretty ‚ÄúSources‚Äù block with doc type + pages.\n",
    "2. `_suggest_for_types()` ‚Üí smart Suggested Questions list based on detected doc types.\n",
    "3. `summary_handler()` ‚Üí injects a user ‚ÄúWhat‚Äôs the summary?‚Äù bubble and appends the LLM summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z6U63lHyoUTd"
   },
   "outputs": [],
   "source": [
    "# ---------- helpers (no style forcing) ----------\n",
    "def _render_sources(sources):\n",
    "    if not sources:\n",
    "        return \"\"\n",
    "    lines = [\"\\n\\nüìç **Sources:**\"]\n",
    "    for s in sources:\n",
    "        pages = s.get(\"pages\", \"\")\n",
    "        lines.append(f\"‚Ä¢ {s.get('doc_type','')} (Pages {pages})\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def summary_handler(history, doc_filter):\n",
    "    \"\"\"User bubble + bot summary; no bullet/style constraints.\"\"\"\n",
    "    if not store.ready:\n",
    "        return history + [[None, \"üìö Please upload and process a PDF first.\"]]\n",
    "    history = history + [[\"üìù What's the summary?\", None]]\n",
    "    resp = store.summarize()\n",
    "    text = (resp.get(\"answer\",\"\") or \"\").strip() + _render_sources(resp.get(\"sources\", []))\n",
    "    history[-1][1] = text\n",
    "    return history\n",
    "\n",
    "def _suggest_for_types(types: list[str]) -> list[str]:\n",
    "    if not types: return []\n",
    "    s = []\n",
    "    if \"Pay Slip\" in types:\n",
    "        s += [\"What is the net pay?\", \"How many working days are shown?\", \"List earnings and deductions.\"]\n",
    "    if \"Contract\" in types:\n",
    "        s += [\"Summarize the key obligations.\", \"What are the termination conditions?\", \"Who are the parties and dates?\"]\n",
    "    if \"Lender Fee Sheet\" in types:\n",
    "        s += [\"What are the total lender fees?\", \"List origination and underwriting fees.\", \"What is the APR and term?\"]\n",
    "    if \"Invoice\" in types:\n",
    "        s += [\"What is the total amount due?\", \"List line items and amounts.\"]\n",
    "\n",
    "    # Removed generic \"brief summary\" since the button already covers it\n",
    "    s += [\"What pages are most relevant to payment amounts?\"]\n",
    "\n",
    "    seen, out = set(), []\n",
    "    for q in s:\n",
    "        if q not in seen:\n",
    "            seen.add(q); out.append(q)\n",
    "    return out[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SzvQSsH6Akm"
   },
   "source": [
    "## üîß Handlers (Process / Chat / Reset)\n",
    "1. process_pdf_handler() ‚Üí runs ingestion, fills ‚ÄúDocument Info‚Äù and ‚ÄúSuggested Questions.‚Äù\n",
    "2. chat_handler() ‚Üí calls store.query() (note the correct kwarg filter_type), prints sources + confidence, and shows friendly errors in-chat if something fails.\n",
    "3. clear_all() ‚Üí resets app state and UI controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "IUyMy6-boakY"
   },
   "outputs": [],
   "source": [
    "# ---------- store ----------\n",
    "# Assumes LocalDocStore() class is defined above (with .process_pdf, .query, .summarize, .structure)\n",
    "store = LocalDocStore()\n",
    "\n",
    "def process_pdf_handler(pdf_file):\n",
    "    if pdf_file is None:\n",
    "        return \"‚ö†Ô∏è Please upload a PDF.\", \"\", gr.update(choices=[\"All\"], value=\"All\"), gr.update(choices=[], value=None)\n",
    "    ok, st = store.process_pdf(pdf_file, filename=getattr(pdf_file, \"name\", \"document.pdf\"))\n",
    "    if ok:\n",
    "        status = (\n",
    "            f\"‚úÖ **Processed**\\n\"\n",
    "            f\"- üìÑ File: {st['filename']}\\n\"\n",
    "            f\"- üìë Pages: {st['pages']}\\n\"\n",
    "            f\"- üìö Docs: {st['docs']}\\n\"\n",
    "            f\"- üß© Chunks: {st['chunks']}\\n\"\n",
    "            f\"- üè∑ Types: {', '.join(st['types'])}\\n\"\n",
    "            f\"- ‚è± Time: {st['time']}\\n\"\n",
    "        )\n",
    "        struct = \"\\n\".join([f\"‚Ä¢ **{d['type']}** (Pages {d['pages']}) ‚Äî {d['chunks']} chunks\" for d in store.structure()])\n",
    "        types = [\"All\"] + st[\"types\"]\n",
    "        suggs = _suggest_for_types(st[\"types\"])\n",
    "        return status, struct, gr.update(choices=types, value=\"All\"), gr.update(choices=suggs, value=(suggs[0] if suggs else None))\n",
    "    return \"‚ùå Error\", \"\", gr.update(choices=[\"All\"], value=\"All\"), gr.update(choices=[], value=None)\n",
    "\n",
    "def chat_handler(message, history, doc_filter, auto_route, num_chunks):\n",
    "    \"\"\"No formatting hints; pass message through as-is, with safe error reporting.\"\"\"\n",
    "    if not store.ready:\n",
    "        return history + [[message, \"üìö Please upload and process a PDF first.\"]]\n",
    "\n",
    "    filt = None if doc_filter == \"All\" else doc_filter\n",
    "\n",
    "    try:\n",
    "        # ‚úÖ Use the correct kwarg name expected by LocalDocStore.query\n",
    "        res = store.query(\n",
    "            message,\n",
    "            k=num_chunks,\n",
    "            filter_type=filt,          # <-- was filter_doc_type (bug)\n",
    "            auto_route=bool(auto_route and filt is None)\n",
    "        )\n",
    "\n",
    "        reply = res.get(\"answer\", \"\").strip()\n",
    "        if res.get(\"sources\"):\n",
    "            reply += \"\\n\\nüìç **Sources:**\\n\" + \"\\n\".join(\n",
    "                f\"‚Ä¢ {s['doc_type']} (Pages {s['pages']}) ‚Äî Relevance {s['relevance']}\"\n",
    "                for s in res[\"sources\"]\n",
    "            )\n",
    "        conf = res.get(\"confidence\")\n",
    "        reply += (\n",
    "            f\"\\n\\n*Confidence: {conf:.1%} | Backend: Mistral-7B (open-source, local)*\"\n",
    "            if conf is not None\n",
    "            else \"\\n\\n*Backend: Mistral-7B (open-source, local)*\"\n",
    "        )\n",
    "        return history + [[message, reply]]\n",
    "\n",
    "    except Exception as e:\n",
    "        # Show a helpful error in the chat instead of a red 'Error' badge\n",
    "        return history + [[message, f\"‚ö†Ô∏è Error answering: `{type(e).__name__}: {e}`\"]]\n",
    "\n",
    "def clear_all():\n",
    "    global store\n",
    "    store = LocalDocStore()\n",
    "    return (None, \"‚è≥ Waiting for PDF upload...\", \"\", gr.update(choices=[\"All\"], value=\"All\"), [], \"\", True, gr.update(choices=[], value=None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IU4eenuq6Foi"
   },
   "source": [
    "## üé® Custom CSS (Handlee Everywhere)\n",
    "1. Imports Google Font ‚ÄúHandlee‚Äù and applies it app-wide (chat, headers, inputs, buttons).\n",
    "2. Defines soft card look, readable chat, warm orange primary buttons, and a dark header band with white title / subtitle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "TqDTUsSxofmd"
   },
   "outputs": [],
   "source": [
    "# ---------- polished, personalized UI (Display Name + Suggestions) ----------\n",
    "APP_TITLE = \"Sriram‚Äôs Document Intelligence Chatbot\"\n",
    "APP_SUBTITLE = \"Private, Fast, and Explainable PDF Q&A\"\n",
    "\n",
    "custom_css = \"\"\"\n",
    "/* Load the handwriting font and apply it APP-WIDE */\n",
    "@import url('https://fonts.googleapis.com/css2?family=Handlee&display=swap');\n",
    "\n",
    "/* ---------- Make Handlee the default everywhere ---------- */\n",
    ":root, body, .gradio-container,\n",
    ".prose, .prose *, .gr-markdown, .gr-markdown *,\n",
    "#chatbox, #chatbox *, .wrap, .message,\n",
    "label, .label, legend, summary,\n",
    "input, textarea, select, option,\n",
    ".gr-textbox textarea, .gr-text-input input,\n",
    ".gr-dropdown, .gr-dropdown *, .gr-select, .gr-select *,\n",
    ".gr-checkbox, .gr-checkbox *, .gr-radio, .gr-radio *,\n",
    ".gr-button, .gr-button *, button, .btn, .btn * {\n",
    "  font-family: \"Handlee\", cursive !important;\n",
    "  color:#0f172a !important; /* readable dark text */\n",
    "}\n",
    "\n",
    "/* ---------- Pleasant card look + readable chat ---------- */\n",
    ".card{\n",
    "  background:#f8fafc;\n",
    "  border-radius:12px;\n",
    "  padding:12px 16px;\n",
    "  box-shadow:0 2px 6px rgba(0,0,0,.05);\n",
    "}\n",
    "#chatbox{\n",
    "  border-radius:12px;\n",
    "  border:1px solid #e2e8f0;\n",
    "  background:#ffffff;\n",
    "  color:#0f172a;\n",
    "  line-height:1.6;\n",
    "  letter-spacing:.1px;\n",
    "  box-shadow:0 4px 12px rgba(2,6,23,.04);\n",
    "}\n",
    "\n",
    "/* ---------- Inputs / text areas / dropdowns ---------- */\n",
    "input[type=\"text\"], input[type=\"search\"], textarea,\n",
    ".gr-textbox textarea, .gr-text-input input{\n",
    "  background:#ffffff !important;\n",
    "  color:#0f172a !important;\n",
    "  border:1px solid #e2e8f0 !important;\n",
    "  border-radius:10px !important;\n",
    "}\n",
    "input::placeholder, textarea::placeholder{ color:#64748b !important; }\n",
    "\n",
    ".gr-dropdown [role=\"combobox\"], .gr-dropdown input,\n",
    ".gr-dropdown .container, .gr-select .container{\n",
    "  background:#ffffff !important;\n",
    "  color:#0f172a !important;\n",
    "  border:1px solid #e2e8f0 !important;\n",
    "  border-radius:10px !important;\n",
    "}\n",
    ".gr-dropdown [role=\"listbox\"], .gr-dropdown .menu{\n",
    "  background:#ffffff !important;\n",
    "  color:#0f172a !important;\n",
    "  border:1px solid #e2e8f0 !important;\n",
    "}\n",
    "\n",
    "/* ---------- Buttons (warm orange primary) ---------- */\n",
    ".gr-button.primary, .gr-button--primary, button.primary{\n",
    "  background:#fb923c !important;         /* orange-400 */\n",
    "  border:1px solid #f97316 !important;    /* orange-500 */\n",
    "  color:#1f2937 !important;               /* slate-700 */\n",
    "  border-radius:12px !important;\n",
    "}\n",
    ".gr-button.primary:hover, .gr-button--primary:hover, button.primary:hover{\n",
    "  background:#f97316 !important;\n",
    "}\n",
    "\n",
    "/* ---------- Header band (white text) ---------- */\n",
    ".header-wrap{\n",
    "  text-align:center;\n",
    "  padding:12px;\n",
    "  background:#1e293b;\n",
    "  border-radius:12px;\n",
    "  margin-bottom:8px;\n",
    "}\n",
    ".header-wrap h1, .header-wrap p, .header-wrap, .header-wrap *{\n",
    "  color:#ffffff !important;\n",
    "}\n",
    "\n",
    "/* ---------- Footer ---------- */\n",
    ".footer{\n",
    "  text-align:center;\n",
    "  color:#64748b;\n",
    "  font-size:13px;\n",
    "  padding:8px 0 0 0;\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cx_e5Xtj6I5X"
   },
   "source": [
    "## üß± Gradio Layout (Blocks)\n",
    "Three-column layout:\n",
    "- Left: Display Name + PDF viewer + Process/Reset\n",
    "- Middle: Document Info, Retrieval Settings, Suggested Questions\n",
    "- Right: Chatbot, Textbox + Send, Generate Summary button above Clear Chat\n",
    "\n",
    "## ‚ö° Wiring the Events\n",
    "1. Live header rename when Display Name changes.\n",
    "2. Button / Linkage:\n",
    "- Process / Change PDF ‚Üí handlers\n",
    "- Send/Enter ‚Üí chat_handler\n",
    "- Generate Summary ‚Üí summary_handler\n",
    "- Ask Selected ‚Üí routes through chat_handler\n",
    "- Reset / Clear Chat ‚Üí state clear\n",
    "\n",
    "## üöÄ Launch\n",
    "Starts the Gradio app with your custom CSS and layout; shows a public share link in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "id": "MFvYu6_4onZR",
    "outputId": "211c6382-1385-461c-c5a5-7eee9adfaf6a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-3597212022.py:1: UserWarning: The parameters have been moved from the Blocks constructor to the launch() method in Gradio 6.0: css. Please pass these parameters to launch() instead.\n",
      "  with gr.Blocks(title=APP_TITLE, css=custom_css) as demo:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
      "* Running on public URL: https://f9e753a4336356148e.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://f9e753a4336356148e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with gr.Blocks(title=APP_TITLE, css=custom_css) as demo:\n",
    "    # --- Header ---\n",
    "    header_html = gr.HTML(f\"\"\"\n",
    "    <div class=\"header-wrap\">\n",
    "      <h1>üß† {APP_TITLE} üß†</h1>\n",
    "      <p>{APP_SUBTITLE}</p>\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    with gr.Row():\n",
    "        # ========== Left: PDF + controls ==========\n",
    "        with gr.Column(scale=2):\n",
    "            with gr.Row():\n",
    "                user_name = gr.Textbox(value=\"Sriram\", label=\"ü™™ Display Name\", scale=2)\n",
    "            pdf_in = PDF(label=\"üìÑ PDF Viewer\", interactive=True, height=560, elem_classes=[\"card\"])\n",
    "            with gr.Row():\n",
    "                process_btn = gr.Button(\"‚öôÔ∏è Process PDF\", variant=\"primary\")\n",
    "                clear_btn   = gr.Button(\"üßπ Reset\", variant=\"secondary\")\n",
    "\n",
    "        # ========== Middle: info + retrieval settings ==========\n",
    "        with gr.Column(scale=1):\n",
    "            gr.Markdown(\"### üìä Document Info\", elem_classes=[\"card\"])\n",
    "            status_md    = gr.Markdown(\"‚è≥ Waiting for PDF upload...\", elem_classes=[\"card\"])\n",
    "            structure_md = gr.Markdown(\"\", elem_classes=[\"card\"])\n",
    "\n",
    "            gr.Markdown(\"### üîç Retrieval Settings\", elem_classes=[\"card\"])\n",
    "            doc_filter   = gr.Dropdown(choices=[\"All\"], value=\"All\", label=\"üè∑ Doc Type Filter\", elem_classes=[\"card\"])\n",
    "            auto_route   = gr.Checkbox(value=False, label=\"üéØ Auto-Route Queries to Likely Doc Type\", elem_classes=[\"card\"])\n",
    "            num_chunks   = gr.Slider(1, 10, value=4, step=1, label=\"üìä Chunks to Retrieve\", elem_classes=[\"card\"])\n",
    "\n",
    "            sugg_dd      = gr.Dropdown(choices=[], value=None, label=\"üí° Suggested Questions\", elem_classes=[\"card\"])\n",
    "            ask_sugg     = gr.Button(\"‚û°Ô∏è Ask Selected\", size=\"sm\")\n",
    "\n",
    "        # ========== Right: chat ==========\n",
    "        with gr.Column(scale=2):\n",
    "            gr.Markdown(\"### üí¨ Ask Questions\", elem_classes=[\"card\"])\n",
    "            bot = gr.Chatbot(height=560, show_label=False, elem_id=\"chatbox\")\n",
    "            with gr.Row():\n",
    "                msg      = gr.Textbox(placeholder=\"e.g., What are the payment terms?\")\n",
    "                send_btn = gr.Button(\"üì§ Send\", variant=\"primary\")\n",
    "\n",
    "            # Summary button ABOVE Clear Chat\n",
    "            summary_btn = gr.Button(\"üìù Generate Summary\", size=\"sm\")\n",
    "\n",
    "            # Clear chat row (stays below)\n",
    "            with gr.Row():\n",
    "                clear_chat = gr.Button(\"üßº Clear Chat\", size=\"sm\")\n",
    "\n",
    "    # --- Footer ---\n",
    "    gr.HTML(\"\"\"\n",
    "    <div class=\"footer\">\n",
    "      Built with ‚ù§Ô∏è using <b>Mistral-7B</b> + <b>Gradio</b> ‚Äî runs locally for privacy\n",
    "    </div>\n",
    "    \"\"\")\n",
    "\n",
    "    # ---------- Handlers (defined inside Blocks so we can bind right away) ----------\n",
    "    def _update_header(name: str):\n",
    "        title = f\"{name.strip()}'s Document Intelligence Chatbot\" if name.strip() else APP_TITLE\n",
    "        return gr.update(value=f\"\"\"\n",
    "        <div class=\"header-wrap\">\n",
    "          <h1>üß† {title}</h1>\n",
    "          <p>{APP_SUBTITLE}</p>\n",
    "        </div>\n",
    "        \"\"\")\n",
    "\n",
    "    def _ask_selected(q, history, _doc_filter, _auto_route, _num_chunks):\n",
    "        if not q:\n",
    "            return history + [[None, \"‚ÑπÔ∏è Pick a suggestion from the left dropdown.\"]]\n",
    "        # Reuse your main chat handler\n",
    "        return chat_handler(q, history, _doc_filter, _auto_route, _num_chunks)\n",
    "\n",
    "    # ---------- Event wiring (ALL inside the Blocks context) ----------\n",
    "    user_name.change(_update_header, inputs=user_name, outputs=header_html)\n",
    "\n",
    "    process_btn.click(process_pdf_handler, inputs=pdf_in,\n",
    "                      outputs=[status_md, structure_md, doc_filter, sugg_dd])\n",
    "    pdf_in.change(process_pdf_handler, inputs=pdf_in,\n",
    "                  outputs=[status_md, structure_md, doc_filter, sugg_dd])\n",
    "\n",
    "    clear_btn.click(clear_all,\n",
    "                    outputs=[pdf_in, status_md, structure_md, doc_filter, bot, msg, auto_route, sugg_dd])\n",
    "    clear_chat.click(lambda: [], outputs=bot)\n",
    "\n",
    "    msg.submit(chat_handler, inputs=[msg, bot, doc_filter, auto_route, num_chunks], outputs=bot) \\\n",
    "       .then(lambda: \"\", None, msg)\n",
    "    send_btn.click(chat_handler, inputs=[msg, bot, doc_filter, auto_route, num_chunks], outputs=bot) \\\n",
    "            .then(lambda: \"\", None, msg)\n",
    "\n",
    "    summary_btn.click(summary_handler, inputs=[bot, doc_filter], outputs=bot)\n",
    "\n",
    "    ask_sugg.click(_ask_selected, inputs=[sugg_dd, bot, doc_filter, auto_route, num_chunks], outputs=bot)\n",
    "\n",
    "# Launch outside the Blocks context\n",
    "demo.launch(share=True, debug=False)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
